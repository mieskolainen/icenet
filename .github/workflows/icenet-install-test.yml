# Github actions CI (continuous integration) setup.
#
# m.mieskolainen@imperial.ac.uk, 2024

name: icenet-install-test

on: [push, workflow_dispatch]

jobs:
  icenet-install-test:
    runs-on: ubuntu-22.04
    steps:
      
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v1

      - uses: actions/setup-python@v4
        with:
          python-version: '3.10.11'
      
      # Free unnecessary utils
      - name: Free Disk Space
        run: |
          echo "Disk space before cleanup:"
          df -h
          sudo rm -rf /usr/local/.ghcup
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/lib/android/sdk/ndk
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo apt-get clean
          echo "Disk space after cleanup:"
          df -h

      ## Install GPU tools
      # conda install -c conda-forge cudatoolkit==11.8.0 cudatoolkit-dev==11.8.0 cudnn=8.9.7.29
      
      # Install conda packages
      
      # (needed e.g. for xgboost tree visualization)
      # conda install python-graphviz
      
      - name: Install pip-requirements
        run: |
          pip install -r requirements-github-actions-1.txt
          pip install -r requirements-github-actions-2.txt
      
      ## Docs building
      - name: Build docs
        run: |
          if [ -e yarn.lock ]; then
          yarn install --frozen-lockfile
          elif [ -e package-lock.json ]; then
          npm ci
          else
          npm i
          fi
          npm run build
          make -C docs/ clean
          make -C docs/ html
          touch docs/build/html/.nojekyll

      ## Release to gh-pages
      - name: Release docs to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/build/html/
      
      ## Unit tests
      - name: Unit tests
        run: |
          pytest icenet/tools/stx.py -rP
          pytest icenet/tools/supertune.py -rP
          pytest icefit/cortools.py -rP
      # pytest icefit/transport.py -rP (gets cancelled by github-actions due to CPU limit)
      
      ## Integration tests
      - name: Deep Learning system integration test (clone stash)
        run: |
          git clone --depth 1 https://github.com/mieskolainen/actions-stash.git
      
      #
      - name: Peakfit (single)
        run: |
          source setenv-github-actions.sh && python icefit/peakfit.py --analyze --group --test_mode --fit_type single --output_name single
      
      #
      - name: Peakfit (dual)
        run: |
          source setenv-github-actions.sh && python icefit/peakfit.py --analyze --group --test_mode --fit_type dual --output_name dual
      
      #
      - name: Peakfit (dual-unitary-I)
        run: |
          source setenv-github-actions.sh && python icefit/peakfit.py --analyze --group --test_mode --fit_type dual-unitary-I --output_name dual-unitary-I
      
      #
      - name: Peakfit (dual-unitary-II)
        run: |
          source setenv-github-actions.sh && python icefit/peakfit.py --analyze --group --test_mode --fit_type dual-unitary-II --output_name dual-unitary-II
      
      #
      - name: Deep Learning system (runme_hnl)
        run: |
          source setenv-github-actions.sh && maxevents=1000; source tests/runme_hnl.sh
          echo "yes" | source superclean.sh
      
      #
      - name: Deep Learning system (runme_eid)
        run: |
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_eid.sh
      
      #
      - name: Deep Learning system (runme_eid_deep)
        run: |
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_eid_deep.sh
      
      #
      - name: Deep Learning system (runme_eid_visual)
        run: |
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_eid_visual.sh
      
      # (This is run twice to test cache files)
      - name: Deep Learning system (runme_brem)
        run: |
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_brem.sh
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_brem.sh
          echo "yes" | source superclean.sh
      
      #
      - name: Deep Learning system (runme_brem_reweight)
        run: |
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_brem_reweight.sh
          echo "yes" | source superclean.sh

      - name: Deep Learning system (runme_zee)
        run: |
          source setenv-github-actions.sh && maxevents=1000; source tests/runme_zee.sh
          echo "yes" | source superclean.sh
      
      - name: Deep Learning system (runme_zee_mixed)
        run: |
          source setenv-github-actions.sh && maxevents=1000; source tests/runme_zee_mixed.sh
          echo "yes" | source superclean.sh

      # (This would require Condor cluster in general, but this imitates)
      #
      #- name: Deep Learning system (runme_zee_gridtune)
      #  run: |
      #    source setenv-github-actions.sh
      #    maxevents=100
          
      #    GRID_ID=-1; GRID_NODES=1 
      #    source tests/runme_zee_gridtune.sh
          
      #    GRID_ID=0; GRID_NODES=1
      #    source tests/runme_zee_gridtune.sh
          
      #    echo "yes" | source superclean.sh
      
      #
      - name: Deep Learning system (runme_trg)
        run: |
          source setenv-github-actions.sh && maxevents=10000; source tests/runme_trg.sh
          echo "yes" | source superclean.sh
        
        ## source setenv-github-actions.sh && maxevents=10000; source tests/runme_brk.sh
        ## source setenv-github-actions.sh && maxevents=10000; source tests/runme_dqcd_vector_train.sh
        ## source setenv-github-actions.sh && maxevents=10000; source tests/runme_hgcal_cnd.sh
        ## source setenv-github-actions.sh && maxevents=10000; source tests/runme_hgcal_trk.sh

