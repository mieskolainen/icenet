# ** Model save and load parameters **
EVAL_PARAM: &EVAL_PARAM
  
  tensorboard: true

  # Training and validation
  savemode: 3                  # Save and evaluate model file every n-th epoch (integer)
                               # (BDT will save always only the last one because it contains all epochs)
  eval_batch_size: 4096
  
  # Post-training evaluation
  readmode: -1                 # specific epoch (int), -1 finds the best loss model, -2 takes the last epoch model
  readmode_metric: 'loss'      # e.g. 'loss', 'AUC' ... [currently only 'loss']
  readmode_operator: 'argmin'  # 


# ----------------------------------------------------------------
# Note for the custom losses with ICEBOOST
# 
# For Hessian (2nd order grad descent) control:
# 
#   ':hessian:constant:1.0'   (DEFAULT)
#   ':hessian:iterative:0.9'  (Iterative, with the moving average parameter)
#   ':hessian:hutchinson:10'  (Hutchinson, with the number of MC samples)
#   ':hessian:exact'          (Exact autograd)
# 
# Usage:
#   objective: 'custom:binary_cross_entropy:hessian:iterative:0.9'
# ----------------------------------------------------------------


## MVA models

# "Stage 1" model
#
# https://xgboost.readthedocs.io/en/latest/parameter.html
iceboost3D:
  train:   'xgb'
  predict: 'xgb_logistic'
  label:   'ICEBOOST3D'
  raytune:  xgb_setup_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 550          # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'             # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'           # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.05
    gamma: 1.5
    max_depth: 13
    min_child_weight: 1.0
    max_delta_step: 1.0
    subsample: 1
    
    colsample_bytree:  1.0
    colsample_bylevel: 1.0
    colsample_bynode:  1.0
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # Special optimization
  opt_param:
    noise_reg: 0.1                # Scheduled noise regularization
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  # Sliced Wasserstein distance [use with custom:binary_cross_entropy and custom:sliced_wasserstein]
  #SWD_param:
  #  beta: 1.0e-3
  #  p: 1                  # p-norm (1,2, ...)
  #  num_slices: 500       # Number of MC projections (Higher the better) 
  #  mode: 'SWD'           # 'SWD' (basic), 'EBSW' (see icefit/transport.py)
  #  max_N: 400000         # Max events limit (400k & 500 slices works with 32 GB Nvidia V100)
  #  var: ['.*']           # Use all variables in SWD
  #  norm_weights: False   # Normalization enforced (if False, model should learn to keep the normalization)
  
  plot_trees: false
  
  <<: *EVAL_PARAM

# ------------------------------------------------
# "Stage 2" models

# Plain XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb_scalar'
  label:   'XGB'
  raytune:  xgb_setup_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 550          # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'             # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'           # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.08
    gamma: 1.5
    max_depth: 13
    min_child_weight: 1.0
    max_delta_step: 1.0
    subsample: 1

    colsample_bytree:  0.9
    colsample_bylevel: 0.9
    colsample_bynode:  0.9
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'  # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']      # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # Special optimization
  opt_param:
    noise_reg: 0.1                # Scheduled noise regularization
  
  plot_trees: false
  
  <<: *EVAL_PARAM


# ICEBOOST with custom loss [BCE]
# 
iceboost0: &ICEBOOST0
  train:   'xgb'
  predict: 'xgb_logistic'
  label:   'ICEBOOST'
  raytune:  xgb_setup_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 550     # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'        # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'      # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1       # Learning rate (slower -> more conservative updates)
    gamma: 1.5               # Minimum loss reduction required to make a further partition on a leaf node
    max_depth: 13            # Maximum depth of a tree (too high may overfit)
    min_child_weight: 1.0    # Minimum sum of instance weight (hessian) needed in a child (higher -> more conservative)
    max_delta_step: 1.0      # Constraint on the maximum change in the model's weights
    subsample: 1             # Subsample ratio of the training instance (=1 uses all data)
    
    colsample_bytree:  0.9   # Specifies the fraction of features to be randomly selected for each tree
    colsample_bylevel: 0.9   # As 'bytree' but applied to individual tree levels
    colsample_bynode:  0.9   # The fraction of features to be randomly selected for each split in a tree
    
    reg_lambda: 2.0          # L2 reg. A penalty to the weights of the features to prevent overfitting
    reg_alpha: 0.05          # L1 reg. A penalty to the absolute value of the weights to induce sparsity
    
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...

  # Special optimization
  opt_param:
    noise_reg: 0.1           # Scheduled noise regularization
  
  # BCE loss domains [use with custom:binary_cross_entropy]
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  #    label_eps: 0.05           # label smoothing epsilon (regularization)
  
  plot_trees: false
  
  <<: *EVAL_PARAM


# ICEBOOST with custom loss [BCE + Sliced Wasserstein]
# 
iceboost_swd:

  <<: *ICEBOOST0
  
  label: 'ICEBOOST-SWD'
  
  bootstrap: 3

  # BCE loss domains [use with custom:binary_cross_entropy]
  BCE_param:
    main:
      beta: 1.0
      classes: [0,1]
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
      label_eps: 0.0     # label smoothing epsilon (regularization)
  
  # Sliced Wasserstein distance [use with custom:binary_cross_entropy and custom:sliced_wasserstein]
  SWD_param:
    beta: 2.0e-2
    p: 1                   # p-norm (1,2, ...)
    num_slices: 500        # Number of MC projections (Higher the better) 
    mode: 'SWD'            # 'SWD' (basic), 'EBSW' (see icefit/transport.py)
    max_N: 400000          # Max events limit (400k & 500 slices works with 32 GB Nvidia V100)
    var: ['.*']            # Use all variables in SWD
    #var: ['fixedGridRhoAll', 'probe_eta', 'probe_pt'] # Use specific ones
    norm_weights: False    # Normalization enforced (if False, model should learn to keep the normalization)


# -----------------------------------------------------------------------------
# Remember to use 'zscore-weighted' (or 'zscore') typically with Neural Networks,
# however, performance with BDTs may be better without.
# -----------------------------------------------------------------------------

## Lipschitz MLP
#
lzmlp0: &LZMLP
  train:   'torch_generic'
  predict: 'torch_scalar'
  raw_logit: true               # Return raw logits

  label:   'LZMLP'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'lzmlp'
  out_dim: 1   # We want to use sigmoid 1D-output model, comment out for default softmax multiclass

  model_param:
    mlp_dim: [96, 96, 96]             # hidden layer dimensions
    activation: 'silu'
    layer_norm: True
    batch_norm: False                 # normalization layers & dropout can be ill-posed here (operators not 1-to-1 compatible with weighted events)
    dropout: 0.01
    act_after_norm: True
  
  # Optimization
  opt_param:  
    #lossfunc: 'binary_cross_entropy'  # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    lossfunc: 'binary_Lq_entropy'     
    q: 0.85                           # Lq exponent (q < 1 -> high density vals emphasized, q > 1 then low emphasized) 
    
    SWD_beta: 1.0e-4                  # Sliced Wasserstein [reweighting regularization]
    SWD_p: 1                          # p-norm (1,2,..), 1 perhaps more robust
    SWD_num_slices: 1000              # Number of MC projections (higher the better)
    SWD_mode: 'SWD'                   # 'SWD' (basic)
    SWD_norm_weights: True            # Normalization enforced

    lipschitz_beta:    5.0e-5         # lipschitz regularization (use with 'lzmlp')
    #logit_L1_beta: 1.0e-2            # logit norm reg. ~ beta * torch.sum(|logits|)
    logit_L2_beta: 5.0e-3             # logit norm reg. ~ beta * torch.sum(logits**2)
    
    noise_reg: 0.1                    # Scheduled noise regularization

    #gamma: -0.5                      # focal_entropy "exponent"
    #temperature: 1                   # logit_norm_cross_entropy "temperature"
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 200
    batch_size: 4096
    lr: 4.0e-4
    weight_decay: 1.0e-2       # L2-regularization
  
  # Scheduler
  scheduler_param:

    #type: 'cos'
    #period: 30
    
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 80             # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


## Lipschitz MLP
#
lzmlp0_nozero:

  <<: *LZMLP

  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  exclude_MVA_vars: ['probe_pfChargedIso', 'probe_ecalPFClusterIso', 'probe_trkSumPtHollowConeDR03', 'probe_trkSumPtSolidConeDR04']
  
  label:   'LZMLP-NOZERO'


## FastKAN
#
fastkan0: &FASTKAN
  train:   'torch_generic'
  predict: 'torch_scalar'
  raw_logit: true               # Return raw logits

  label:   'FASTKAN'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'fastkan'
  out_dim: 1   # We want to use sigmoid 1D-output model, comment out for default softmax multiclass
  
  model_param:
    grid_min: -2.0                    # Activation learning param
    grid_max:  2.0                    # 
    num_grids:   8                    # 
    mlp_dim: [96, 96, 96]             # hidden layer dimensions
    use_base_update: True             # Use "base MLP" in addition
    
    last_tanh: True                   # Extra tanh layer
    last_tanh_scale: 10.0             # Scale after tanh()
  
  # Optimization
  opt_param:  
    #lossfunc: 'binary_cross_entropy'  # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    lossfunc: 'binary_Lq_entropy'    # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    q: 0.85                           # Lq exponent (q < 1 -> high density vals emphasized, q > 1 then low emphasized) 
    
    SWD_beta: 1.0e-4                  # Sliced Wasserstein [reweighting regularization]
    SWD_p: 1                          # p-norm (1,2,..), 1 perhaps more robust
    SWD_num_slices: 1000              # Number of MC projections (higher the better)
    SWD_mode: 'SWD'                   # 'SWD' (basic)
    SWD_norm_weights: True            # Normalization enforced
    
    #lipshitz_beta: 1.0e-4            # Lipshitz regularization (use with 'lzmlp')
    #logit_L1_beta: 1.0e-2            # logit norm reg. ~ beta * torch.sum(|logits|)
    logit_L2_beta: 5.0e-3             # logit norm reg. ~ beta * torch.sum(logits**2)

    noise_reg: 0.1                    # Scheduled noise regularization

    #gamma: 2                         # focal_entropy "exponent"
    #temperature: 1                   # logit_norm_cross_entropy "temperature"
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 200
    batch_size: 4096
    lr: 4.0e-4
    weight_decay: 1.0e-2       # L2-regularization
  
  # Scheduler
  scheduler_param:
    #type: 'cos'
    #period: 30
    
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 80              # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
      
  <<: *EVAL_PARAM

  # Deploy (or test) mode device
  deploy_device: 'cpu'         # 'auto', 'cpu', 'cuda'


## FastKAN
#
fastkan0_nozero:

  <<: *FASTKAN
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  exclude_MVA_vars: ['probe_pfChargedIso', 'probe_ecalPFClusterIso', 'probe_trkSumPtHollowConeDR03', 'probe_trkSumPtSolidConeDR04']
  
  label:   'FASTKAN-NOZERO'


## Deep MLP
#
dmlp0: &DMLP
  train:   'torch_generic'
  predict: 'torch_scalar'
  raw_logit: true               # Return raw logits

  label:   'DMLP'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'dmlp'
  out_dim: 1   # We want to use sigmoid 1D-output model, comment out for default softmax multiclass
  
  model_param:
    mlp_dim: [96, 96, 96]             # hidden layer dimensions
    activation: 'silu'
    layer_norm: True
    batch_norm: False                 # normalization layers & dropout can be ill-posed here (operators not 1-to-1 compatible with weighted events)
    dropout: 0.01
    act_after_norm: True
    
    skip_connections: False
    last_tanh: True                   # Extra tanh layer
    last_tanh_scale: 10.0             # Scale after tanh()
  
  # Optimization
  opt_param:
    #lossfunc: 'binary_cross_entropy' # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    lossfunc: 'binary_Lq_entropy'
    q: 0.85                            # Lq exponent (q < 1 -> high density vals emphasized, q > 1 then low emphasized) 
    
    SWD_beta: 1.0e-4                  # Sliced Wasserstein [reweighting regularization]
    SWD_p: 1                          # p-norm (1,2,..), 1 perhaps more robust
    SWD_num_slices: 1000              # Number of MC projections (higher the better)
    SWD_mode: 'SWD'                   # 'SWD' (basic)
    SWD_norm_weights: True            # Normalization enforced

    #logit_L1_beta: 1.0e-2            # logit norm reg. ~ lambda * torch.sum(|logits|)
    logit_L2_beta: 5.0e-3             # logit norm reg. ~ lambda * torch.sum(logits**2)
    
    noise_reg: 0.1                    # Scheduled noise regularization

    #gamma: 2                         # focal_entropy "exponent"
    #temperature: 1                   # logit_norm_cross_entropy "temperature"
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 200
    batch_size: 4096
    lr: 4.0e-4
    weight_decay: 1.0e-2       # L2-regularization
  
  # Scheduler
  scheduler_param:
    #type: 'cos'
    #period: 30
    
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 80             # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
    
  <<: *EVAL_PARAM


## Deep MLP
#
dmlp0_nozero:

  <<: *DMLP

  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  exclude_MVA_vars: ['probe_pfChargedIso', 'probe_ecalPFClusterIso', 'probe_trkSumPtHollowConeDR03', 'probe_trkSumPtSolidConeDR04']
  
  label:   'DMLP-NOZERO'


## Deep Normalizing Flow
#
dbnf0:
  train:   'flow'
  predict: 'torch_flow'
  label:   'DBNF0'
  raytune:  null

  # Gradient descent
  opt_param:
    lossfunc: 'flow_logpx'
    optimizer: 'Adam'
    clip_norm: 0.1
    
    epochs: 250
    batch_size: 512           # Keep it high!
    lr: 4.0e-4
    weight_decay: 0.0         # L2-regularization (0 seems OK) 
    polyak: 0.998

    start_epoch: 0
  
  # Learning rate reduction on plateau
  scheduler_param:  
    factor:  0.1
    patience: 20
    cooldown: 10
    min_lr: 0.0005
    early_stopping: 100

  # Model structure
  model_param:  
    flows: 10                 # number of flow blocks
    layers: 0                 # intermediate layers in a flow block
    hidden_dim: 10            # 
    residual: 'gated'         # choises 'none', 'normal', 'gated'
    perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM
