# DQCD tune0.yml
#
rootname: 'dqcd'
rngseed: 123456                      # Fixed seed for training data mixing

# ----------------------------------------------------
mva_param: &MVA_INPUT_PARAM
  use_conditional: false             # Conditional (theory parametric) input
  num_classes: 2                     # Number of classes in MVA
  signalclass: 1                     # Signal class ID
  
  inputvar:         null
  inputvar_scalar: 'MVA_SCALAR_VARS' # Input variables, implemented under mvavars.py
  inputvar_jagged: 'MVA_JAGGED_VARS'

  # Needs to be alphabetical
  jagged_maxdim:
    
    Jet:   10
    Muon:  10
    sv:    10
    cpf:   10
    npf:   10

  frac: 0.5                          # Train vs validate/test split fraction
  
  # Variable imputation
  imputation_param:
    active: true                     # True / False
    var: null                        # Array of variables to be imputated, if null, then all
    algorithm: 'constant'            # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
    fill_value: -999.0               # For constant imputation
    knn_k: 8                         # Number of nearest neighbours considered
    values: null                     # Special values which indicate the need for imputation, if null, then only Inf/Nan

  varnorm: 'zscore'                  # Variable normalization: 'zscore', 'madscore', null
  #varnorm_tensor: 'zscore'          # Tensor variable normalization
  #varnorm_graph: null               # Not implemented yet
  
  # Graph object construction
  graph_param:
    global_on: True
    self_loops: True
    directed: False
    coord: 'pxpypze'                 # 'ptetaphim', 'pxpypze'

  # ** Image tensor object construction **
  image_param:

    # See the corresponding construction under common.py
    channels: 2                 # 1,2,...
    
    # bin-edges
    eta_bins: []
    phi_bins: []
    

# ----------------------------------------------------
genesis_runmode:
  
  maxevents:  null
  inputmap: "mc_input.yml"
  tree_name:  null #'ntuplizer/tree'

  targetfunc:  null                  # Training target,    implemented under mctargets.py
  filterfunc: 'filter_standard'      # Training filtering, implemented under mcfilter.py
  cutfunc:    'cut_fiducial'         # Basic cuts,         implemented under cuts.py

  xcorr_flow: True                   # Full N-point correlations computed between cuts

# ----------------------------------------------------
train_runmode:

  <<: *MVA_INPUT_PARAM

  maxevents: null
  
  input_tag: null
  model_tag: null

  ## Reweighting setup
  reweight: true
  reweight_param: &REWEIGHT_PARAM
    
    maxevents: 1000000             # Maximum number of events for the PDF construction
    equal_frac: True               # Equalize integrated class fractions
    differential: False
    reference_class: 0             # Reference class: 0 = (background), 1 = (signal), 2 = (another class) ..., 
    
    # Note about asymmetry: Keep the reference class as 0 == background, because signal
    # MC has much longer tail over pt. Thus, it is not possible to re-weight background
    # to signal very easily, but it is possible to re-weight signal to background.
    # This is seen by inspecting the re-weighted distributions with small and large statistics.
    
    # ---------------------
    # Differential parameters
    
    # dimension: 'pseudo-2D'         # '2D', 'pseudo-2D', '1D'
    # pseudo_type: 'product'         # 'product', 'geometric_mean'
    
    # var_A: 'x_hlt_pt'
    # var_B: 'x_hlt_eta'

    # bins_A:  [0.0001, 100.0, 150]  # Make sure the bounds cover the phase space
    # binmode_A:  'linear'           # 'log10' or 'linear' binning
    
    # bins_B: [-3.1, 3.1, 150]
    # binmode_B: 'linear'
    
    # # ! Variable, and binning min/max boundaries are both transformed !
    # transform_A: 'log10'           # 'log10', 'sqrt', 'square', null
    # transform_B: null
    
    # max_reg: 10000.0               # Maximum weight cut-off regularization


  ## Outlier protection in the training
  outlier_param:
    algo: 'truncate'   # algorithm: 'truncate', null
    qmin: 0.01         # in [0,100] 
    qmax: 99.9         # in [0,100]


  # ** Activate models here **
  # Give all models some unique identifier and label
  models:  !include configs/dqcd/models.yml
  active_models: &ACTIVE_MODELS
    
    #- gnet1
    #- gnet2
    #- gnet3

    - xgb0
    
    #- dmlp
    #- deps
    - vae0    
    #- dbnf0
    - lgr0
    
    #- xgb1
    #- dmlp
    #- dmx0
    #- dmx1
    
    - cut0
    - cut1
    - cut2
    - cutset0
  
  raytune: !include configs/dqcd/raytune.yml

  # Distillation training
  # -- the order must be compatible with the causal order in 'active_models'
  distillation:

    # Big, sophisticated model
    source:
      #xgb0
      
    # Simple, compressed models
    drains:
      #- xgb1
      # - add more here

  # Batched "deep" training
  batch_train_param:
    blocksize: 150000   # Maximum number of events simultaneously in RAM
    epochs: 50          # Number of global epochs (1 epoch = one iteration over the full input dataset), same for all models
    #num_cpu: null      # Set null for auto, or an integer for manual.

# ----------------------------------------------------
eval_runmode:
  
  <<: *MVA_INPUT_PARAM

  maxevents: null

  input_tag: null
  model_tag: null

  reweight: true
  reweight_param: *REWEIGHT_PARAM

  models:  !include configs/dqcd/models.yml
  active_models: *ACTIVE_MODELS

# ----------------------------------------------------
plot_param: !include configs/dqcd/plots.yml
