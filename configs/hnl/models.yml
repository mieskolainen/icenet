FINAL_STATE_FILTER: &FINAL_STATE_FILTER

  filter[0]:
    operator: null   # null (unary), 'cartesian_and', 'cartesian_or'
    sets: [0]        # input set ids (one or several)
  
  # ... more filters here

  # -------------------------------------------------
  
  set[0]:
    expand: 'powerset' # 'set', 'vetoset', or 'powerset' which goes through all boolean vector combinations
    cutset:
      # Latex description is for boolean [0,1] per cut
      [{latex: ['$\mu', '$e'],   cut: 'BOOL@leadingLeptons_isElectron == True'},
       {latex: ['\mu',  'e'],    cut: 'BOOL@subleadingLeptons_isElectron == True'},
       {latex: ['OS',   'SS'],   cut: 'dilepton_charge > 0'},
       {latex: ['mer$', 'iso$'], cut: 'nominal_dR_l2j > 0.4'}]
  
  # note: dataframe has SS for dilepton_charge == 1, OS for == -1
  # -- = -2 -> SS
  # -+ =  0 -> OS
  # +- =  0 -> OS
  # ++ =  2 -> SS

  # ... more sets here

# -----------------------------------------------------------------------

# -----------------------------------------------
# ** Mutual Information regularization **
MI_REG_PARAM: &MI_REG_PARAM
  
  classes: [0]             #  Which classes to use in the regularization
  
  losstype: 'MINE_EMA'     # 'MINE', 'MINE_EMA', 'DENSITY', 'DCORR', 'PEARSON' (use PEARSON only for DEBUG)
  min_count: 32            #  Minimum number of events (per category)
  max_N: null              #  Maximum number of events per function call (use to limit sample size)
  min_score: null          #  Consider only events with MVA output larger than this value (null for None)
  poisson_weight: True     #  per category Poisson sqrt(N) weighted loss
  
  # ------------------
  # Neural MI param

  eval_batch_size: 4096    #  Evaluation batch size (pure memory <-> time tradeoff)
  alpha:  0.01             #  Exponential Moving Average (EMA) coupling parameter
  ma_eT:  [null]           #  EMA tracking values
  
  y_dim: [1]               #  dimensionality ([1] for NN scalar output Z vs target X)
  
  epochs: 30
  batch_size: 512
  lr: 1.0e-3
  weight_decay: 1.0e-5
  clip_norm: 1.0
  
  mlp_dim: [128, 128]
  batch_norm: False
  dropout: 0.01
  noise_std: 0.025
  activation: 'relu'
  
  # ------------------

  set_filter: *FINAL_STATE_FILTER
# -----------------------------------------------


## MVA models

cutset0:
  train:   'cutset'
  predict: 'cutset'
  
  label:   'cutset'
  raytune:  null
  
  # Using yaml multiline suntax without last linebreak syntax with >-
  # https://stackoverflow.com/questions/3790454/how-do-i-break-a-string-in-yaml-over-multiple-lines
  cutstring: >-
    x_hlt_pms2 < 10000 &&  
    x_hlt_invEInvP < 0.2 &&  
    x_hlt_trkDEtaSeed < 0.01 &&  
    x_hlt_trkDPhi < 0.2 && 
    x_hlt_trkChi2 < 40 && 
    x_hlt_trkValidHits >= 5 && 
    x_hlt_trkNrLayerIT >= 2


cut0:
  train:    'cut'
  predict:  'cut'
  label:    '(-1) x m_llj'
  variable: 'nominal_m_llj'
  sign: -1
  transform: null

cut1:
  train:    'cut'
  predict:  'cut'
  label:    'tagger_score'
  variable: 'tagger_score'
  sign: 1
  transform: null


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune:  xgb_trial_0
  
  # ** Custom set of variables **
  exclude_MVA_vars: ['tagger_score']
  
  # booster parameters
  model_param:
    num_boost_round: 50       # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'       # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.67
    max_depth: 8
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 4.8            # L2 regularization
    reg_alpha: 0.05            # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'    # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']        # for custom losses, otherwise 'logloss', 'mlogloss' ...

  plot_trees: False
  
  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0-red:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB(m_llj)'
  raytune:  xgb_trial_0
  
  # ** Custom set of variables **
  exclude_MVA_vars: ['.*']
  include_MVA_vars: ['nominal_m_llj']
  
  # booster parameters
  model_param:
    num_boost_round: 10       # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'     # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.67
    max_depth: 8
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1
    
    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 4.8          # L2 regularization
    reg_alpha: 0.05          # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'          # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']              # for evaluation, 'logloss', 'mlogloss'
  
  plot_trees: False
  
  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb-MI-min:
  train:   'xgb'
  predict: 'xgb_logistic'     # We apply logistic link function manually (because custom loss)
  label:   'XGB-MI-min'
  raytune:  xgb_trial_0

  # ** Custom set of variables **
  exclude_MVA_vars: ['tagger_score']
  
  # booster parameters
  model_param:
    num_boost_round: 50       # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'     # 'auto', 'cpu', 'cuda'

    # booster parameters
    learning_rate: 0.05
    gamma: 1.67
    max_depth: 8
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 4.8          # L2 regularization
    reg_alpha: 0.05          # L1 regularization
    
    # learning task parameters
    #objective: 'binary:logistic'            # Note that 'multi:softprob' does not work with distillation
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  MI_param:
    beta: [2.0]                # Positive for minimizing
    <<: *MI_REG_PARAM
  
  plot_trees: False
  
  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb-MI-max:
  train:   'xgb'
  predict: 'xgb_logistic'     # We apply logistic link function manually (because custom loss)
  label:   'XGB-MI-max'
  raytune:  xgb_trial_0

  # ** Custom set of variables **
  exclude_MVA_vars: ['tagger_score']
  
  # booster parameters
  model_param:
    num_boost_round: 50       # number of epochs (equal to the number of trees!)
  
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'     # 'auto', 'cpu', 'cuda'

    # booster parameters
    learning_rate: 0.05
    gamma: 1.67
    max_depth: 8
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 4.8          # L2 regularization
    reg_alpha: 0.05          # L1 regularization
    
    # learning task parameters
    #objective: 'binary:logistic'            # Note that 'multi:softprob' does not work with distillation
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  MI_param:
    beta: [-2.0]       # Negative for maximizing MI
    <<: *MI_REG_PARAM

  plot_trees: False
  
  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MLP
dmlp0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP'
  raytune:  null
  
  # ** Custom set of variables **
  exclude_MVA_vars: ['tagger_score']
  
  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [32, 32]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
  
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'  # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                   # focal_entropy exponent
    temperature: 1             # logit norm temperature
    
    optimizer: 'Adam'
    clip_norm: 1.0
    
    epochs: 50
    batch_size: 256
    lr: 3.0e-4
    weight_decay: 0.00001      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MLP
dmlp0-red:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP(m_llj)'
  raytune:  null

  # ** Custom set of variables **
  exclude_MVA_vars: ['.*']
  include_MVA_vars: ['nominal_m_llj']

  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [32, 32]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
  
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'  # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                   # focal_entropy exponent
    temperature: 1             # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 1.0
    
    epochs: 50
    batch_size: 256
    lr: 3.0e-4
    weight_decay: 0.00001      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch

# Deep MLP
dmlp-MI-min:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP-MI-min'
  raytune:  null

  # ** Custom set of variables **
  exclude_MVA_vars: ['tagger_score']
  
  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [32, 32]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
    
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 1.0
    
    epochs: 50
    batch_size: 256
    lr: 3.0e-4
    weight_decay: 0.00001      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1
  
  # Mutual Information regularization (comment out if not used)
  # Remember to use small enough learning rate in the total loss optimizer!
  MI_param:
    beta: [0.99]               # Positive for minimizing MI between (x,y), Negative for Maximizing
    <<: *MI_REG_PARAM
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MLP
dmlp-MI-max:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP-MI-max'
  raytune:  null

  # ** Custom set of variables **
  exclude_MVA_vars: ['tagger_score']
  
  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [32, 32]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
  
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 1.0
    
    epochs: 50
    batch_size: 256
    lr: 3.0e-4
    weight_decay: 0.00001      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1
  
  # Mutual Information regularization (comment out if not used)
  # Remember to use small enough learning rate in the total loss optimizer!
  MI_param:
    beta:  [-0.99]      #  Positive for minimizing MI between (x,y), Negative for Maximizing (too |high| can make it unstable)
    <<: *MI_REG_PARAM

  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch
