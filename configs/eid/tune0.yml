# Electron ID tune0.yml
#
# General comments:
#
# - Keep the minibatch size small enough, otherwise weak convergence to optima with some models
# - Other imputation methods than 'constant' not natively compatible with re-weighting (-> biased)

rootname: 'eid'
rngseed: 123456                      # Fixed seed for training data mixing

# ----------------------------------------------------
mva_param: &MVA_INPUT_PARAM
  use_conditional: false             # Conditional (theory parametric) input
  num_classes: 2                     # Number of classes in MVA
  signalclass: 1                     # Signal class ID
  
  inputvar:         'CMSSW_MVA_SCALAR_VARS_ORIG'
  inputvar_scalar:  null             # 'MVA_SCALAR_VARS' # Input variables, implemented under mvavars.py
  inputvar_jagged:  null             # 'MVA_JAGGED_VARS'
  jagged_maxdim:    6

  varnorm: 'zscore'                  # Variable normalization: 'zscore', 'madscore', null
  varnorm_tensor: 'zscore'           # Tensor variable normalization
  #varnorm_graph: null               # Not implemented yet
  
  frac: 0.9                          # Train vs validate/test split fraction
  
  # Imputation
  imputation_param:
    active: true                      # True / False
    var: 'CMSSW_MVA_SCALAR_VARS_ORIG' # Array of variables to be imputated
    algorithm: 'constant'             # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
    fill_value: 0                     # For constant imputation
    knn_k: 8                          # Number of nearest neighbours considered
    values: [-999, -666, -10]         # Special values which indicate the need for imputation, if null, then only Inf/Nan are imputed
  
  # Graph object construction
  graph_param:
    global_on: True
    self_loops: True
    directed: False
    coord: 'pxpypze'                  # 'ptetaphim', 'pxpypze'
  
  # ** Image tensor object construction **
  image_param:

    # See the corresponding construction under common.py
    channels: 2                 # 1,2,...

    # limited range, only [-0.5, 0.5] (values are re-centered)
    eta_bins: [-0.5       , -0.48387097, -0.46774194, -0.4516129 , -0.43548387,
       -0.41935484, -0.40322581, -0.38709677, -0.37096774, -0.35483871,
       -0.33870968, -0.32258065, -0.30645161, -0.29032258, -0.27419355,
       -0.25806452, -0.24193548, -0.22580645, -0.20967742, -0.19354839,
       -0.17741935, -0.16129032, -0.14516129, -0.12903226, -0.11290323,
       -0.09677419, -0.08064516, -0.06451613, -0.0483871 , -0.03225806,
       -0.01612903,  0.        ,  0.01612903,  0.03225806,  0.0483871 ,
        0.06451613,  0.08064516,  0.09677419,  0.11290323,  0.12903226,
        0.14516129,  0.16129032,  0.17741935,  0.19354839,  0.20967742,
        0.22580645,  0.24193548,  0.25806452,  0.27419355,  0.29032258,
        0.30645161,  0.32258065,  0.33870968,  0.35483871,  0.37096774,
        0.38709677,  0.40322581,  0.41935484,  0.43548387,  0.4516129 ,
        0.46774194,  0.48387097,  0.5       ]
    
    # Limited range, only (-pi/2, pi/2) (values are re-centered)
    phi_bins: [-1.57079633, -1.52012548, -1.46945463, -1.41878378, -1.36811293,
       -1.31744208, -1.26677123, -1.21610038, -1.16542953, -1.11475868,
       -1.06408783, -1.01341699, -0.96274614, -0.91207529, -0.86140444,
       -0.81073359, -0.76006274, -0.70939189, -0.65872104, -0.60805019,
       -0.55737934, -0.50670849, -0.45603764, -0.40536679, -0.35469594,
       -0.3040251 , -0.25335425, -0.2026834 , -0.15201255, -0.1013417 ,
       -0.05067085,  0.        ,  0.05067085,  0.1013417 ,  0.15201255,
        0.2026834 ,  0.25335425,  0.3040251 ,  0.35469594,  0.40536679,
        0.45603764,  0.50670849,  0.55737934,  0.60805019,  0.65872104,
        0.70939189,  0.76006274,  0.81073359,  0.86140444,  0.91207529,
        0.96274614,  1.01341699,  1.06408783,  1.11475868,  1.16542953,
        1.21610038,  1.26677123,  1.31744208,  1.36811293,  1.41878378,
        1.46945463,  1.52012548,  1.57079633]


# ----------------------------------------------------
genesis_runmode:

  maxevents:  null
  inputmap: null #"mc_input.yml"
  tree_name:  'ntuplizer/tree'

  targetfunc: 'target_e'              # Training target,    implemented under mctargets.py
  filterfunc: 'filter_no_egamma'      # Training filtering, implemented under mcfilter.py
  cutfunc:    'cut_standard'          # Basic cuts,         implemented under cuts.py

  xcorr_flow: True                    # Full N-point correlations computed between cuts


# ----------------------------------------------------
train_runmode:

  <<: *MVA_INPUT_PARAM

  maxevents: null
  
  input_tag: null
  model_tag: null

  ## Reweighting setup
  reweight: true
  reweight_param: &REWEIGHT_PARAM

    maxevents: 100000             # Maximum number of events for the PDF construction
    equal_frac: true              # Equalize integrated class fractions
    differential: true            # Differential reweighting
    # ---------------------
    
    reference_class: 0            # Reference class: 0 = (background), 1 = (signal), 2 = (another class) ..., 
    # Note about asymmetry: Keep the reference class as 0 == background, because signal
    # MC has much longer tail over pt. Thus, it is not possible to re-weight background
    # to signal very easily, but it is possible to re-weight signal to background.
    # This is seen by inspecting the re-weighted distributions with small and large statistics.

    dimension: 'pseudo-2D'        # '2D', 'pseudo-2D', '1D'
    pseudo_type: 'product'        # 'product', 'geometric_mean'

    var_A:  'trk_pt'
    var_B:  'trk_eta'

    bins_A:  [0.0001, 50.0, 100] # Make sure the bounds cover the phase space
    binmode_A:  'linear'         # 'log10' or 'linear' binning
    
    bins_B: [-3.1, 3.1, 100]     #
    binmode_B: 'linear'
    
    # ! Variable, and binning min/max boundaries are both transformed !
    transform_A: 'log10'         # 'log10', 'sqrt', 'square', null
    transform_B: null
    
    max_reg: 1000.0              # Maximum weight cut-off regularization


  ## Outlier protection in the training
  outlier_param:
    algo: 'truncate'   # algorithm: 'truncate', null
    qmin: 0.01         # in [0,100] 
    qmax: 99.9         # in [0,100]


  # ** Activate models here **
  # Give all models some unique identifier and label
  models:  !include configs/eid/models.yml
  active_models: &ACTIVE_MODELS

    - xgb0
    - gxgb0

    #- dbnf0
    - gnet0
    #- gnet1
    #- gnet2
    #- gnet3
    
    - flr0
    - lgr0
    - dmx0
    - dmlp
    - cnn0
  
  ##  - xtx0
  
  raytune: !include configs/eid/raytune.yml

  # Distillation training
  # -- the order must be compatible with the causal order in 'active_models'
  distillation:

    # Big, sophisticated model
    source:
      xgb0
    
    # Simple, compressed models
    drains:
      - gxgb0
      - gnet3
      - dmx0
      # - add more here
  
  # Batched "deep" training
  batch_train_param:
    blocksize: 150000   # Maximum number of events simultaneously in RAM
    epochs: 50          # Number of global epochs (1 epoch = one iteration over the full input dataset), same for all models
    #num_cpu: null      # Set null for auto, or an integer for manual.

# ----------------------------------------------------
eval_runmode:
  
  <<: *MVA_INPUT_PARAM
  
  maxevents: null

  input_tag: null
  model_tag: null

  reweight: true
  reweight_param: *REWEIGHT_PARAM

  models:  !include configs/eid/models.yml
  active_models: *ACTIVE_MODELS

# ----------------------------------------------------
plot_param: !include configs/eid/plots.yml
