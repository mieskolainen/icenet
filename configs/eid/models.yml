
# Dual graph + XGB network
gxgb0:
  
  train:   'graph_xgb'
  predict: 'graph_xgb'
  label:   'DGXGB'

  # ** GRAPH NETWORK **
  graph:
    train:   'torch_graph'
    predict: 'torch_graph'
    label:   'GXGB-G'
    raytune:  null

    # Convolution model
    conv_type:   'SuperEdgeConv' # See models under icenet/deep/graph.py

    # Model
    model_param:
      task: 'graph'             # 'graph', 'node', 'edge_symmetric', 'edge_asymmetric' (level of inference)
      global_pool: 'mean'
      z_dim: 196                # Convolution output dimension
      
      # Message passing parameters
      conv_MLP_act: 'relu'
      conv_MLP_bn: True
      conv_MLP_dropout: 0.01
      conv_aggr: 'max'
      conv_knn: 8

      fusion_MLP_act: 'relu'
      fusion_MLP_bn: False
      fusion_MLP_dropout: 0.01

      final_MLP_act: 'relu'
      final_MLP_bn:  False
      final_MLP_dropout: 0.01
      
      # Domain adaptation
      DA_active: False
      # ...

    # Optimization
    opt_param:  
      lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
      gamma: 2                  # focal_entropy exponent
      temperature: 1            # logit norm temperature
      
      optimizer: 'Adam'
      clip_norm: 0.1

      epochs: 150
      batch_size: 64
      lr: 1.0e-3
      weight_decay:  0.00005     # L2-regularization

    # Scheduler
    scheduler_param:
      step_size: 200
      gamma: 0.1
    
    device: 'auto'               # alternative 'cpu:0', 'cuda:0'
    num_workers: 4
    
    # Read/Write of epochs
    savemode: 'all'              # 'all', 'latest'
    readmode: -1                 # -1 is the last saved epoch

  # ** XGBOOST **
  xgb:
    train:   'xgb'
    predict: 'xgb'
    label:   'GXGB-XGB'
    raytune:  null

    # general parameters
    model_param:
      num_boost_round: 100    # number of epochs (equal to the number of trees!)
      
      booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
      tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)
      
      # booster parameters
      learning_rate: 0.1
      gamma: 0.0
      max_depth: 10
      min_child_weight: 1.0
      max_delta_step: 1
      subsample: 1

      colsample_bytree:  1
      colsample_bylevel: 1
      colsample_bynode:  1
      
      reg_lambda: 2.0        # L2 regularization
      reg_alpha: 0.0         # L1 regularization
      
      # learning task parameters
      objective:  'binary:logistic'  # Note that 'multi:softprob' does not work with distillation
      eval_metric: ['logloss']       # for evaluation, 'mlogloss'

    plot_trees: False
    
    # Read/Write of epochs
    savemode: 'all'                  # 'all', 'latest'
    readmode: -1                     # -1 is the last saved epoch


# Graph net
gnet0:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'NNConv'
  raytune:  'gnn_setup'

  conv_type: 'NNConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.0

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 0.1

    epochs: 150
    batch_size: 64
    lr: 1.0e-3
    weight_decay:  0.00005      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'             # 'all', 'latest'
  readmode: -1                # 'name', epoch number, or -1 uses the last saved epoch


# Graph net
gnet1:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'EdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'EdgeConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 0.1

    epochs: 150
    batch_size: 64
    lr: 1.0e-3
    weight_decay:  0.00005      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'             # 'all', 'latest'
  readmode: -1                # 'name', epoch number, or -1 uses the last saved epoch


# Graph net
gnet2:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'SuperEdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'SuperEdgeConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 0.1

    epochs: 150
    batch_size: 64
    lr: 1.0e-3
    weight_decay:  0.00005      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'             # 'all', 'latest'
  readmode: -1                # 'name', epoch number, or -1 uses the last saved epoch


# Graph net
gnet3:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'DynamicEdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'DynamicEdgeConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01
    
    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 0.1

    epochs: 150
    batch_size: 64
    lr: 1.0e-3
    weight_decay:  0.00005      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'             # 'all', 'latest'
  readmode: -1                # 'name', epoch number, or -1 uses the last saved epoch


# Factorized Likelihood Ratio
flr0:
  train:   'flr'
  predict: 'flr'
  label:   'FLR'
  raytune:  null

  nbins: 60
  qmin:  0.5 # in [0,100]
  qmax: 99.5 # in [0,100]


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune:  'xgb_setup'

  # general parameters
  model_param:
    num_boost_round: 100    # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)

    # booster parameters
    learning_rate: 0.1
    gamma: 0.0
    max_depth: 10
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  1
    colsample_bylevel: 1
    colsample_bynode:  1
    
    reg_lambda: 2.0       # L2 regularization
    reg_alpha: 0.0        # L1 regularization
    
    # learning task parameters
    objective:  'binary:logistic' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']     # for evaluation, 'mlogloss'

  plot_trees: False

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MLP
dmlp:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP'
  raytune:  null

  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [64, 64, 64]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
    
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 0.1
    
    epochs: 50
    batch_size: 196
    lr: 1.0e-3
    weight_decay: 0.00001     # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1
  
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Logistic Regression (convex model = global optimum guarantee)
lgr0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'LGR'
  raytune:  null

  # Model param
  conv_type: 'lgr'
  model_param:
    null

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'

    clip_norm: 0.1

    epochs: 50
    batch_size: 196
    lr: 1.0e-3
    weight_decay: 0.00001       # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1  

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Convolutional Neural Net + MAXOUT MLP
cnn0:
  train:   'torch_generic'
  predict: 'torch_image_vector'
  label:   'CNN+MAXO'
  raytune:  null
  
  # Model
  conv_type: 'cnn+maxo'
  model_param:
    dropout_cnn: 0.1
    dropout_mlp: 0.1
    mlp_dim: 64

    # Set these according to the input matrix (could autodetect ...)
    nchannels: 2
    nrows:     59
    ncols:     59

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 0.1
    
    epochs: 100
    batch_size: 128
    lr: 1.0e-3
    weight_decay: 0.00         # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1
      
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# XTX (hyperbinned) classifier
xtx0:
  train:   'xtx'
  predict: 'xtx'
  label:   'XTX'
  raytune:  null

  # Choose kinematic binning variables
  binning:
    var:   ['trk_eta', 'trk_pt']
    edges: [[-2.5, -1.5, -1.15, -0.75, 0.0, 0.75, 1.15, 1.5, 2.5],
            [0.5, 0.7, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]]

  # Model param
  conv_type: 'maxo'
  model_param:
    num_units: 2
    neurons:  20
    dropout:  0.5

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'        # Adam, AdamW
    clip_norm: 0.1

    epochs: 50
    batch_size:  196
    lr: 1.0e-3
    weight_decay: 0.01        # L2-regularization

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MAXOUT network
dmx0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'MAXO'
  raytune:  null

  # Model parameter
  conv_type: 'maxo'
  model_param:
    num_units: 8
    neurons:  50
    dropout:  0.01

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'Adam'        # Adam, AdamW
    clip_norm: 0.1

    epochs: 50
    batch_size: 128
    lr: 1.0e-3
    weight_decay: 0.00001       # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep Normalizing Flow
dbnf0:
  train:   'flow'
  predict: 'torch_flow'
  label:   'DBNF'
  raytune:  null

  # Gradient descent
  opt_param:
    lossfunc: 'flow_logpx'
    optimizer: 'Adam'
    clip_norm: 0.1
    
    epochs: 100
    batch_size: 512           # Keep it high!
    lr: 1.0e-3
    weight_decay: 0.0         # L2-regularization
    polyak: 0.998

    start_epoch: 0

  # Learning rate reduction on plateau
  scheduler_param:  
    factor:  0.1
    patience: 20
    cooldown: 10
    min_lr: 0.0005
    early_stopping: 100

  # Model structure
  model_param:  
    flows: 10                 # number of flow blocks
    layers: 0                 # intermediate layers in a flow block
    hidden_dim: 10            # 
    residual: 'gated'         # choises 'none', 'normal', 'gated'
    perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  modelname: 'null'
  tensorboard: 'tensorboard'
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch
