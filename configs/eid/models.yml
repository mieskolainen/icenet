# ** Model save and load parameters **
EVAL_PARAM: &EVAL_PARAM
  
  tensorboard: true

  # Training and validation
  savemode: 1                  # Save and validate every n-th epoch (integer)
  eval_batch_size: 1024
  
  # Post-training evaluation
  readmode: -1                 # specific epoch (int), -1 finds the best loss model, -2 takes the last epoch model
  readmode_metric: 'loss'      # e.g. 'loss', 'AUC' ... [currently only 'loss']
  readmode_operator: 'argmin'  # 


# - Keep the minibatch size small enough, otherwise weak convergence to optima with some models

# Reference BDT (pre-computed in ROOT files)
cut0:
  train:    'cut'
  predict:  'cut'
  label:    'ele_mva_depth15'
  variable: 'ele_mva_value_depth15'
  sign: 1
  transform: null

# Dual graph + XGB network
gxgb0:
  
  train:   'graph_xgb'
  predict: 'graph_xgb'
  label:   'DGXGB'

  # ** GRAPH NETWORK **
  graph:
    train:   'torch_graph'
    predict: 'torch_graph'
    label:   'GXGB-G'
    raytune:  null

    # Convolution model
    conv_type:   'SuperEdgeConv' # See models under icenet/deep/graph.py

    # Model
    model_param:
      task: 'graph'              # 'graph', 'node', 'edge_symmetric', 'edge_asymmetric' (level of inference)
      global_pool: 'mean'
      z_dim: 196                 # Convolution output dimension
      
      # Message passing parameters
      conv_MLP_act: 'relu'
      conv_MLP_bn: True
      conv_MLP_dropout: 0.01
      conv_aggr: 'mean'
      conv_knn: 8

      fusion_MLP_act: 'relu'
      fusion_MLP_bn: False
      fusion_MLP_dropout: 0.01

      final_MLP_act: 'relu'
      final_MLP_bn:  False
      final_MLP_dropout: 0.01
      
      # Domain adaptation
      DA_active: False
      # ...

    # Optimization
    opt_param:  
      lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
      gamma: 2                  # focal_entropy exponent
      temperature: 1            # logit norm temperature
      
      optimizer: 'AdamW'
      clip_norm: 1.0

      epochs: 150
      batch_size: 256
      lr: 1.0e-3
      weight_decay:  1.0e-5     # L2-regularization

    # Scheduler
    scheduler_param:
      type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
      step_size: 200
      gamma: 0.1
    
    device: 'auto'               # alternative 'cpu:0', 'cuda:0'
    num_workers: 4
    
    <<: *EVAL_PARAM
  

  # ** XGBOOST **
  xgb:
    train:   'xgb'
    predict: 'xgb'
    label:   'GXGB-XGB'
    raytune:  null

    # general parameters
    model_param:
      num_boost_round: 100    # number of epochs (equal to the number of trees!)
      
      booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
      tree_method: 'hist'
      device:      'auto'     # 'auto', 'cpu', 'cuda'
      
      # booster parameters
      learning_rate: 0.1
      gamma: 0.0
      max_depth: 10
      min_child_weight: 1.0
      max_delta_step: 1
      subsample: 1

      colsample_bytree:  1
      colsample_bylevel: 1
      colsample_bynode:  1
      
      reg_lambda: 2.0        # L2 regularization
      reg_alpha: 0.0         # L1 regularization
      
      # learning task parameters
      objective:  'binary:logistic'  # Note that 'multi:softprob' does not work with distillation
      eval_metric: ['logloss']       # for evaluation, 'mlogloss'

    plot_trees: false
    
    <<: *EVAL_PARAM


# Graph net
gnet0:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'NNConv'
  raytune:  'gnn_setup'

  conv_type: 'NNConv'         # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'mean'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.0

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0

    epochs: 30
    batch_size: 256
    lr: 3.0e-4
    weight_decay:  1.0e-5      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Graph net
gnet1:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'EdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'EdgeConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'mean'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0

    epochs: 30
    batch_size: 256
    lr: 3.0e-4
    weight_decay:  1.0e-5      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Graph net
gnet2:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'SuperEdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'SuperEdgeConv'  # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'mean'         # 'mean', 'max', 'VariancePreserving', https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#aggregation-operators
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0

    epochs: 30
    batch_size: 256
    lr: 3.0e-4
    weight_decay:  1.0e-5      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Graph net
gnet3:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'DynamicEdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'DynamicEdgeConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'graph'             # 'graph', 'node', 'edge_directed', 'edge_undirected' (level of inference)
    global_pool: 'mean'
    z_dim: 196                # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'mean'
    conv_knn: 8
    
    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01
    
    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0

    epochs: 30
    batch_size: 256
    lr: 3.0e-4
    weight_decay:  1.0e-5      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Factorized Likelihood Ratio
flr0:
  train:   'flr'
  predict: 'flr'
  label:   'FLR'
  raytune:  null

  nbins: 60
  qmin:  0.5 # in [0,100]
  qmax: 99.5 # in [0,100]


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune:  'xgb_setup'

  # general parameters
  model_param:
    num_boost_round: 150    # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'     # 'auto', 'cpu', 'cuda'

    # booster parameters
    learning_rate: 0.1
    gamma: 1.67
    max_depth: 10
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 2.0          # L2 regularization
    reg_alpha: 0.05          # L1 regularization
    
    # learning task parameters
    objective:  'binary:logistic' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']      # for evaluation, 'logloss', 'mlogloss'

  plot_trees: false
  
  <<: *EVAL_PARAM

# -----------------------------------------------------------------------------
# Remember to use 'zscore-weighted' (or 'zscore') typically with Neural Networks,
# however, performance with BDTs may be better without.
# -----------------------------------------------------------------------------

# Deep MLP
dmlp:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP'
  raytune:  null

  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [64, 64, 64]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
    
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 30
    batch_size: 196
    lr: 1.0e-3
    weight_decay: 1.0e-5       # L2-regularization
  
  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Logistic Regression (convex model = global optimum guarantee)
lgr0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'LGR'
  raytune:  null

  # Model param
  conv_type: 'lgr'
  model_param:
    null

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'

    clip_norm: 1.0

    epochs: 10
    batch_size: 196
    lr: 1.0e-3
    weight_decay: 1.0e-5     # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1  

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Convolutional Neural Net + MAXOUT MLP
cnn0:
  train:   'torch_generic'
  predict: 'torch_image_vector'
  label:   'CNN+MAXO'
  raytune:  null
  
  # Model
  conv_type: 'cnn+maxo'
  model_param:
    dropout_cnn: 0.1
    dropout_mlp: 0.1
    mlp_dim: 64

    # Set these according to the input matrix (could autodetect ...)
    nchannels: 2
    nrows:     59
    ncols:     59

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 30
    batch_size: 128
    lr: 1.0e-3
    weight_decay: 1.0e-5     # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1
      
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Deep MAXOUT network
dmx0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'MAXO'
  raytune:  null

  # Model parameter
  conv_type: 'maxo'
  model_param:
    num_units: 8
    neurons:  50
    dropout:  0.01

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'        # Adam, AdamW
    clip_norm: 1.0

    epochs: 30
    batch_size: 128
    lr: 1.0e-3
    weight_decay: 1.0e-5     # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Deep Normalizing Flow
dbnf0:
  train:   'flow'
  predict: 'torch_flow'
  label:   'DBNF'
  raytune:  null

  # Gradient descent
  opt_param:
    lossfunc: 'flow_logpx'
    optimizer: 'Adam'
    clip_norm: 0.1
    
    epochs: 100
    batch_size: 512           # Keep it high!
    lr: 1.0e-3
    weight_decay: 0.0         # L2-regularization (without any weight decay seems OK)
    polyak: 0.998
    
    start_epoch: 0
  
  # Learning rate reduction on plateau
  scheduler_param:  
    factor:  0.1
    patience: 20
    cooldown: 10
    min_lr: 0.0005
    early_stopping: 100

  # Model structure
  model_param:  
    flows: 10                 # number of flow blocks
    layers: 0                 # intermediate layers in a flow block
    hidden_dim: 10            # 
    residual: 'gated'         # choises 'none', 'normal', 'gated'
    perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM
