# ** Model save and load parameters **
EVAL_PARAM: &EVAL_PARAM
  
  tensorboard: true

  # Training and validation
  savemode: 1                  # Save and validate every n-th epoch (integer)
  eval_batch_size: 1024

  # Post-training evaluation
  readmode: -1                 # specific epoch (int), -1 finds the best loss model, -2 takes the last epoch model
  readmode_metric: 'loss'      # e.g. 'loss', 'AUC' ... [currently only 'loss']
  readmode_operator: 'argmin'  # 


# Graph net
gnet0:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'EdgeConv'
  raytune:  'gnn_setup'
  
  conv_type:   'EdgeConv'       # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'edge_symmetric'      # 'graph', 'node', 'edge_symmetric', 'edge_asymmetric' (level of inference)
    global_pool: 'mean'
    z_dim: 32                   # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'          # cross_entropy, focal_entropy, logit_norm_cross_entropy
    negative_sampling: False           # Synthetic negative edge sampling (in edge classification)
    
    gamma: 2                           # focal_entropy exponent
    temperature: 1                     # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 80
    batch_size: 128
    lr: 3.0e-4
    weight_decay:  0.0      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM


# Graph net
gnet1:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'DynamicEdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'DynamicEdgeConv'   # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'edge_symmetric'       # 'graph', 'node', 'edge_symmetric', 'edge_asymmetric' (level of inference)
    global_pool: 'mean'
    z_dim: 32                    # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...
    
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'          # cross_entropy, focal_entropy, logit_norm_cross_entropy
    negative_sampling: False           # Synthetic negative edge sampling (in edge classification)
    
    gamma: 2                           # focal_entropy exponent
    temperature: 1                     # logit norm temperature

    optimizer: 'Adam'
    clip_norm: 1.0

    epochs: 80
    batch_size: 128
    lr: 3.0e-4
    weight_decay:  0.0      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
 
  <<: *EVAL_PARAM


# Graph net
gnet2:
  train:    'torch_graph'
  predict:  'torch_graph'
  label:    'SuperEdgeConv'
  raytune:  'gnn_setup'

  conv_type: 'SuperEdgeConv'     # See models under icenet/deep/graph.py
  
  # Model
  model_param:
    task: 'edge_symmetric'       # 'graph', 'node', 'edge_symmetric', 'edge_asymmetric' (level of inference)
    global_pool: 'mean'
    z_dim: 32                    # Convolution output dimension
    
    # Message passing parameters
    conv_MLP_act: 'relu'
    conv_MLP_bn: True
    conv_MLP_dropout: 0.01
    conv_aggr: 'max'
    conv_knn: 8

    fusion_MLP_act: 'relu'
    fusion_MLP_bn: False
    fusion_MLP_dropout: 0.01

    final_MLP_act: 'relu'
    final_MLP_bn:  False
    final_MLP_dropout: 0.01

    # Domain adaptation
    DA_active: False
    # ...

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'          # cross_entropy, focal_entropy, logit_norm_cross_entropy
    negative_sampling: False           # Synthetic negative edge sampling (in edge classification)
    
    gamma: 2                           # focal_entropy exponent
    temperature: 1                     # logit norm temperature

    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 80
    batch_size: 128
    lr: 3.0e-4
    weight_decay:  0.0      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  <<: *EVAL_PARAM
